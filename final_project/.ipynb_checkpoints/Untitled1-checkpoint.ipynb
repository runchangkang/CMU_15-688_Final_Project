{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NBA GAME PREDICTION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Project for 15-688 Practical Data Science\n",
    "#### Authors: Runchang Kang / Zheng Luo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The charming part of sport games must be the unpredictability. Countless of buzzer beaters and upsets not only crazied the fans, but also raised human's curiosity to use machine to predict the result. As basketball fans and students who just took the practical data science course, we can't wait to use the knowledge and skillsets that learnt from the course to play with the \"mistical power\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Content\n",
    "\n",
    "In this project, we will show the pipeline of using data science techniques to play with data and try to get the best prediction as we can. The main structure of the project includes:\n",
    "- [Data Collection](#Data-Collection)\n",
    "- [Data Processing](#Data-Processing)\n",
    "- [Model Selection](#Model-Selection)\n",
    "- [Training and Prediction](#Training-and-Prediction)\n",
    "- [Iteration and Adjustment](#Iteration-and-Adjustment)\n",
    "- [Conclusion](#Conclusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pickle\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although many datasets already exist online, many of them are poorly structured and hard to utilize. We decide to scrape the raw data from a NBA statistic website [https://www.basketball-reference.com/] The website has documented every single game orderly so it won't be too hard to get the data we need. \n",
    "\n",
    "Before starting scraping the website, we have to think about what kind of data we need and how the data will be structured locally. After a heated discussion, we figured out there is no perfect way to do the prediction. Due to the large number of poteintial influential factors, such as, the player's physical and emotional state, the team chemistry, the strength of schedule, the home or road game factor, the influence from social media, the player's fluctuation during the game, and so on. We have to admit the existence of bias.\n",
    "\n",
    "We planned to collect the statistic data of each game each team from 1995 to 2017, and use the mean of previous `5 games' data` to represent the general status of a team. However, due to the opponents of previous five games are different, we assume that the mean will neutralize the difference of level of those opponents. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We scrape all teams all games statistic from 1995 to 2017. However, some teams changed their name or location within the year, we have to carefully process these data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utlis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEAM_NAMES=[\"ATL\",\"BOS\",\"BRK\",\"NJN\",\"CHA\",\"CHH\",\"CHO\",\"CHI\",\"CLE\",\"DAL\",\"DEN\",\"DET\",\"GSW\",\"HOU\",\"IND\",\"LAC\",\"LAL\",\"MEM\",\"VAN\",\"MIA\",\"MIL\",\"MIN\",\"NOP\",\"NOH\",\"NOK\",\"NYK\",\"OKC\",\"SEA\",\"ORL\",\"PHI\",\"PHO\",\"POR\",\"SAC\",\"SAS\",\"TOR\",\"UTA\", \"WSB\",\"WAS\"]\n",
    "YEARS=[\"%s\" %x for x in range(1995,2017)]\n",
    "BASE_TEAM_URL= \"https://www.basketball-reference.com/teams\"\n",
    "#several teams have multiple names\n",
    "TEAM_INDEX={\"ATL\":0,\n",
    " \"BOS\":1,\n",
    " \"BRK\":2,\"NJN\":2,\n",
    " \"CHA\":3,\"CHH\":3,\"CHO\":3,\n",
    " \"CHI\":4,\n",
    " \"CLE\":5,\n",
    " \"DAL\":6,\n",
    " \"DEN\":7,\n",
    " \"DET\":8,\n",
    " \"GSW\":9,\n",
    " \"HOU\":10,\n",
    " \"IND\":11,\n",
    " \"LAC\":12,\n",
    " \"LAL\":13,\n",
    " \"MEM\":14,\"VAN\":14,\n",
    " \"MIA\":15,\n",
    " \"MIL\":16,\n",
    " \"MIN\":17,\n",
    " \"NOH\":18,\"NOK\":18,\"NOP\":18,\n",
    " \"NYK\":19,\n",
    " \"SEA\":20,\"OKC\":20,\n",
    " \"ORL\":21,\n",
    " \"PHI\":22,\n",
    " \"PHO\":23,\n",
    " \"POR\":24,\n",
    " \"SAC\":25,\n",
    " \"SAS\":26,\n",
    " \"TOR\":27,\n",
    " \"UTA\":28,\n",
    " \"WSB\":29,\"WAS\":29}\n",
    "\n",
    "#this function helps create structural dataframe for each team\n",
    "#we store the data as pickle file for each team\n",
    "def createNewDataframes():\n",
    "    return (pd.DataFrame(columns = [\"teamIndex\",\"oppoIndex\",\"gameDate\",\"result\",\"FG\",\"FGA\",\"3P\",\"3PA\",\"FT\",\"FTA\",\"ORB\",\"DRB\",\"AST\",\"STL\",\"BLK\",\"TOV\",\"PF\",\"PTS\"]),\n",
    "           pd.DataFrame(columns = [\"teamIndex\",\"oppoIndex\",\"gameDate\",\"result\",\"TS%\",\"eFG%\",\"3PAr\",\"FTr\",\"ORB%\",\"DRB%\",\"TRB%\",\"AST%\",\"STL%\",\"BLK%\",\"TOV%\",\"USB%\",\"ORtg\",\"DRtg\"]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping\n",
    "\n",
    "The scaping took about four hours to run. We seperated the works into 3 laptops and stored all the data as pickle file which named by team.\n",
    "\n",
    "We store the following features of each game:\n",
    "\n",
    "-`FG : Field Goal`\n",
    "\n",
    "-`FGA : Filed Goal Atempts`\n",
    "\n",
    "-`3P : 3-points Filed Goal`\n",
    "\n",
    "-`3PA : 3-points Filed Goal Atempts`\n",
    "\n",
    "-`FT : Free Throw`\n",
    "\n",
    "-`FTA : Free Throw Atempts`\n",
    "\n",
    "-`ORB : Offence Rebound `\n",
    "\n",
    "-`DRB : Defence Rebound `\n",
    "\n",
    "-`AST : Assists`\n",
    "\n",
    "-`STL : Steals`\n",
    "\n",
    "-`BLK : Block Shot `\n",
    "\n",
    "-`TOV : Turnovers`\n",
    "\n",
    "-`PF : Personal Fouls`\n",
    "\n",
    "-`PTS : Points`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this block of code will scape and store the data as pickle files for each team\n",
    "\n",
    "for team in TEAM_NAMES:\n",
    "    basicDf,advancedDf = createNewDataframes()\n",
    "    for year in YEARS:\n",
    "        url = BASE_TEAM_URL +\"/\"+team+\"/\"+year+\"_games.html\"\n",
    "        print(url)\n",
    "        response = requests.get(url)\n",
    "        root = BeautifulSoup(response.text,\"html5lib\")\n",
    "        items = root.find_all(\"td\",attrs={\"data-stat\": \"box_score_text\",\"class\":\"center\"})\n",
    "        winList = []\n",
    "        if items == None:\n",
    "            continue\n",
    "        #get game results\n",
    "        winItems = root.find_all(\"td\",attrs={\"data-stat\": \"game_result\",\"class\":\"center\"})\n",
    "        winList = []\n",
    "        for i in winItems:\n",
    "            if i.text==\"W\":\n",
    "                winList.append(1)\n",
    "            elif i.text ==\"L\":\n",
    "                winList.append(0)\n",
    "        \n",
    "        if len(items)!= len(winList):\n",
    "            print(\"mismatch!\")\n",
    "        #get opponents:\n",
    "        oppoList = []\n",
    "        oppoItems = root.find_all(\"td\",attrs={\"data-stat\": \"opp_name\",\"class\":\"left\"})\n",
    "        \n",
    "        \n",
    "        for i in oppoItems:\n",
    "            tmp = i[\"csk\"][:3]\n",
    "            if tmp != None:\n",
    "                oppoList.append(tmp)\n",
    "            else:\n",
    "                print(\"here is a None!\")\n",
    "        \n",
    "        if len(items)!= len(winList):\n",
    "            print(\"mismatch!\")\n",
    "        for index,item in enumerate(items):\n",
    "            newUrl = BASE_TEAM_URL[:-6]+item.a[\"href\"]\n",
    "            date = item.a[\"href\"][11:19]\n",
    "            basicList = [TEAM_INDEX[team],TEAM_INDEX[oppoList[index]],date,winList[index]]\n",
    "            advancedList = [TEAM_INDEX[team],TEAM_INDEX[oppoList[index]],date,winList[index]]\n",
    "            newResponse = requests.get(newUrl)\n",
    "            tmpRoot = BeautifulSoup(newResponse.text,\"html5lib\")\n",
    "            idString = \"all_box_%s_basic\" % team.lower()\n",
    "            tmpItem = tmpRoot.find(\"div\",attrs={\"id\":idString}).find(\"tfoot\")\n",
    "            basicList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"fg\"}).text)\n",
    "            basicList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"fga\"}).text)\n",
    "            basicList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"fg3\"}).text)\n",
    "            basicList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"fg3a\"}).text)\n",
    "            basicList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"ft\"}).text)\n",
    "            basicList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"fta\"}).text)\n",
    "            basicList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"orb\"}).text)\n",
    "            basicList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"drb\"}).text)\n",
    "            basicList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"ast\"}).text)\n",
    "            basicList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"stl\"}).text)\n",
    "            basicList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"blk\"}).text)\n",
    "            basicList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"tov\"}).text)\n",
    "            basicList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"pf\"}).text)\n",
    "            basicList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"pts\"}).text)\n",
    "            basicDf.loc[basicDf.shape[0]] = basicList\n",
    "            \n",
    "            \n",
    "            idString = \"all_box_%s_advanced\" % team.lower()\n",
    "            tmpItem = tmpRoot.find(\"div\",attrs={\"id\":idString}).find(\"tfoot\")\n",
    "            advancedList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"ts_pct\"}).text)\n",
    "            advancedList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"efg_pct\"}).text)\n",
    "            advancedList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"fg3a_per_fga_pct\"}).text)\n",
    "            advancedList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"fta_per_fga_pct\"}).text)\n",
    "            advancedList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"orb_pct\"}).text)\n",
    "            advancedList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"drb_pct\"}).text)\n",
    "            advancedList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"trb_pct\"}).text)\n",
    "            advancedList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"ast_pct\"}).text)\n",
    "            advancedList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"stl_pct\"}).text)\n",
    "            advancedList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"blk_pct\"}).text)\n",
    "            advancedList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"tov_pct\"}).text)\n",
    "            advancedList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"usg_pct\"}).text)\n",
    "            advancedList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"off_rtg\"}).text)\n",
    "            advancedList.append(tmpItem.find(\"td\",attrs={\"data-stat\":\"def_rtg\"}).text)\n",
    "            advancedDf.loc[advancedDf.shape[0]] = advancedList\n",
    "        basicName = \"%s_basic_data.pickle\"%team\n",
    "        advancedName = \"%s_advanced_data.pickle\"%team\n",
    "        basicDf.to_pickle(basicName)\n",
    "        advancedDf.to_pickle(advancedName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utlis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#manipulate the dataframe into right data type\n",
    "def changeDtypes(df):\n",
    "    df['gameDate'] = pd.to_datetime(df['gameDate'], format=\"%Y%m%d\")\n",
    "    for i in df.columns:\n",
    "        if i not in ('teamIndex','gameDate','result',\"oppoIndex\"):\n",
    "            df[i]=df[i].astype(\"float64\") \n",
    "        \n",
    "    return df\n",
    "\n",
    "#calculate the mean of previous k games,ignore the columns in a\n",
    "#adding one more row of previous result's mean\n",
    "def refactorDF2(df,k,a):\n",
    "    newDf = df.copy()\n",
    "    for i in range(k,df.shape[0]):\n",
    "        \n",
    "        for column in a:\n",
    "            newDf.at[i,column] = df.iloc[i-k:i-1][column].mean() \n",
    "        newDf.at[i,\"preResult\"] = df.iloc[i-k:i-1][\"result\"].astype(\"float64\").mean() \n",
    "    return newDf.iloc[k:]\n",
    "\n",
    "#calculate the mean of previous k games,ignore the columns in a\n",
    "#adding one more row of previous result's mean\n",
    "def refactorDF(df,k,a):\n",
    "    newDf = df.copy()\n",
    "    for i in range(k,df.shape[0]):\n",
    "        \n",
    "        for column in a:\n",
    "            newDf.at[i,column] = df.iloc[i-k:i-1][column].mean() \n",
    "        \n",
    "    return newDf.iloc[k:]\n",
    "#get the column names that need to process\n",
    "def getBasicColumnList(df):\n",
    "    a=list(df.columns)\n",
    "    for i in ['teamIndex','gameDate','result',\"oppoIndex\",\"preResult\"]:\n",
    "        a.remove(i)\n",
    "    return a\n",
    "#get the column names that need to process\n",
    "def getBasicColumnList2(df):\n",
    "    a=list(df.columns)\n",
    "    for i in ['teamIndex','gameDate','result',\"oppoIndex\"]:\n",
    "        a.remove(i)\n",
    "    return a\n",
    "#helper function for adding the up to date seasonal win records\n",
    "def compareDate(df):\n",
    "    newDf = df.copy()\n",
    "    DELTALimit = datetime.timedelta(weeks=16)\n",
    "    newDf.at[0,\"preResult\"] = 0\n",
    "    newSeasonIndex = 0\n",
    "    for i in range(1,df.shape[0]):\n",
    "        delta = newDf.at[i,\"gameDate\"]- newDf.at[i-1,\"gameDate\"]\n",
    "        \n",
    "        if delta < DELTALimit:\n",
    "            newDf.at[i,\"preResult\"] = df.iloc[newSeasonIndex:i][\"result\"].astype(\"float64\").sum()/(i-newSeasonIndex)\n",
    "        else:\n",
    "            \n",
    "            newSeasonIndex = i\n",
    "            newDf.at[i,\"preResult\"] = 0\n",
    "    return newDf\n",
    "\n",
    "\n",
    "basicDataFileNames = [\"%s_basic_data.pickle\"%x for x in TEAM_INDEX]\n",
    "advancedDataFileNames = [\"%s_advanced_data.pickle\"%x for x in TEAM_INDEX]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this function combine all of the data that we scraped form the stat website\n",
    "#including considering the changed of team names\n",
    "#it takes some time to process, I store them in a file\n",
    "def combineData(DataFileNames,k=5):\n",
    "    dataFrameSet = dict()\n",
    "    for fileName in DataFileNames:\n",
    "        df = pd.read_pickle(fileName)\n",
    "        df = changeDtypes(df)\n",
    "        df = compareDate(df)\n",
    "        df = refactorDF(df,k,ignoreCol)\n",
    "        \n",
    "        curIndex = df.iloc[0][\"teamIndex\"]\n",
    "        if curIndex not in dataFrameSet:\n",
    "            dataFrameSet[curIndex] = df\n",
    "        else:\n",
    "            dataFrameSet[curIndex] = pd.concat([dataFrameSet[curIndex],df])\n",
    "    return dataFrameSet\n",
    "\n",
    "\n",
    "dataFrameSet = combineData(advancedDataFileNames)\n",
    "\n",
    "with open(\"join_data.pickle\",\"wb\") as f:\n",
    "    pickle.dump(dataFrameSet,f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this function create all of the features in the dataframe and output as \n",
    "#numpy array\n",
    "def createTheFeatures(dataFrameSet,ignoreCol):\n",
    "    finalArray = np.array([])\n",
    "    flag = True\n",
    "    yValue = np.array([])\n",
    "    for key in dataFrameSet:\n",
    "        df = dataFrameSet[key]\n",
    "        \n",
    "        for i in range(df.shape[0]):\n",
    "            tmpArray = df.iloc[i][ignoreCol].values\n",
    "            tmpArray = tmpArray.reshape(1,tmpArray.shape[0])\n",
    "            if np.isnan(np.min(tmpArray)):\n",
    "                continue\n",
    "            oppoDf = dataFrameSet[df.iloc[i][\"oppoIndex\"]]\n",
    "            time = df.iloc[i][\"gameDate\"]\n",
    "            oppoRow = oppoDf.loc[oppoDf[\"gameDate\"]==time]\n",
    "            if not oppoRow.empty:\n",
    "                \n",
    "                oppoArray = oppoRow[ignoreCol].values\n",
    "                if np.isnan(np.min(oppoArray)):\n",
    "                    continue\n",
    "                tmpArray = np.append(tmpArray,oppoArray)\n",
    "                if flag:\n",
    "                    finalArray = tmpArray\n",
    "                    flag = False\n",
    "                else:   \n",
    "                    finalArray = np.vstack((finalArray,tmpArray))\n",
    "                yValue = np.append(yValue,df.iloc[i][\"result\"])\n",
    "                print(finalArray.shape)\n",
    "                print(yValue.shape)\n",
    "    return finalArray,yValue\n",
    "\n",
    "\n",
    "with open(\"join_data.pickle\",\"rb\") as f:\n",
    "    dataFrameSet=pickle.load(f)\n",
    "ignoreCol = getBasicColumnList2(dataFrameSet[0])\n",
    "X,y = createTheFeatures(dataFrameSet,ignoreCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#store the data into files\n",
    "with open(\"X_ad3.pickle\",\"wb\") as f:\n",
    "    pickle.dump(X,f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "f.close()\n",
    "with open(\"y_ad3.pickle\",\"wb\") as f:\n",
    "    pickle.dump(y,f,protocol=pickle.HIGHEST_PROTOCOL)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
